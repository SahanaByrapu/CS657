# -*- coding: utf-8 -*-
"""CS657_preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xO_QDN5vmkLo8hr7NtnfGSBXklp25wdq
"""

"""
!pip install pyspark

from google.colab import drive
drive.mount('/content/gdrive/', force_remount=True)

!unzip gdrive/My\ Drive/optiver-realized-volatility-prediction.zip > /dev/null """
import sys
import os
import math
import time
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession 

# spark config
spark = SparkSession \
    .builder \
    .appName("realized volaitility prediction") \
    .config("spark.driver.maxResultSize", "400g") \
    .config("spark.driver.memory", "400g") \
    .config("spark.executor.memory", "100g") \
    .config("spark.master", "local[12]") \
    .getOrCreate()

data = spark.read.option("header",True) \
     .csv("/user/sbyrapu/input/optiver-realized-volatility-prediction/train.csv")

from pyspark.sql.functions import udf, regexp_replace, lower, split, trim, explode, lit, col, collect_list,concat_ws,abs
from pyspark.sql.window import Window
from pyspark.sql import functions as F
from pyspark.sql.functions import lag   
from pyspark.sql.types import FloatType


unique_stock_ids=data.select('stock_id').distinct().collect()
train=spark.createDataFrame([], data.schema)
test=spark.createDataFrame([], data.schema)
for row in unique_stock_ids:
   r=row['stock_id']
   data_df=data.filter(data.stock_id==r)
   splits = data_df.randomSplit([2.0, 1.0], 24)
   train=train.union(splits[0])
   test=test.union(splits[1])

train=train.withColumn('row_id', concat_ws('-',train.stock_id,train.time_id))
test=test.withColumn('row_id', concat_ws('-',test.stock_id,test.time_id))

train.show()

test.show()

import numpy as np
def calc_wap1(df):

  wap1=(df["bid_price1"] * df['ask_size1'] + df["ask_price1"]*df["bid_size1"])/(df['ask_size1']+df['bid_size1'])

  return wap1

def calc_wap2(df):

  wap2=(df["bid_price2"] * df['ask_size2'] + df["ask_price2"]*df["bid_size2"])/(df['ask_size2']+df['bid_size2'])

  return wap2

def calc_wap3(df):

  wap3=(df["bid_price1"] * df['bid_size1'] + df["ask_price1"]*df["ask_size1"])/(df['ask_size1']+df['bid_size1'])

  return wap3

def calc_wap4(df):

  wap4=(df["bid_price2"] * df['bid_size2'] + df["ask_price2"]*df["ask_size2"])/(df['ask_size2']+df['bid_size2'])

  return wap4

def log_return(series):
   return np.log(series).diff()

def wap_balance(series):
   return np.series.diff()

def realized_volatility(series):
   return np.sqrt(np.sum(series**2)) 

def count_unique(series):
    return len(np.unique(series))

from pyspark.sql.functions import mean as _mean, stddev as _stddev, col,lit

def book_preprocessor(file_path):
     df= spark.read.parquet(file_path)
  
     
     #calculating weighted average prices
     # Calculate Wap
     df= df.withColumn('wap1', calc_wap1(df) )
     df= df.withColumn('wap2', calc_wap2(df) )
     df= df.withColumn('wap3', calc_wap3(df) )
     df= df.withColumn('wap4', calc_wap4(df) )
        
    
     # Calculate wap balance
     df= df.withColumn('wap_balance', abs(df.wap1 - df.wap2))
   
     # Calculate spread
     df=df.withColumn('price_spread',  (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2))
     df=df.withColumn('price_spread2', (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2))
     df=df.withColumn('bid_spread',  df['bid_price1'] - df['bid_price2'])
     df=df.withColumn('ask_spread', df['ask_price1'] - df['ask_price2'])
     df=df.withColumn("bid_ask_spread", abs(df['bid_spread'] - df['ask_spread']))
     df=df.withColumn('total_volume',  (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2']))
     df=df.withColumn('volume_imbalance',abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2'])))
     df = df.toPandas()
     # Calculate log returns
     df['log_return1'] = df.groupby(['time_id'])['wap1'].transform(log_return)
     df['log_return2'] = df.groupby(['time_id'])['wap2'].transform(log_return)
     df['log_return3'] = df.groupby(['time_id'])['wap3'].transform(log_return)
     df['log_return4'] = df.groupby(['time_id'])['wap4'].transform(log_return)

     # Dict for aggregations
     create_feature_dict = {
          'wap1':[np.sum, np.std],
          'wap2': [np.sum, np.std],
          'wap3': [np.sum, np.std],
          'wap4': [np.sum, np.std],
          'log_return1': [realized_volatility],
          'log_return2': [realized_volatility],
          'log_return3': [realized_volatility],
          'log_return4': [realized_volatility],
           'wap_balance': [np.sum, np.max],
           'price_spread':[np.sum, np.max],
           'price_spread2':[np.sum, np.max],
           'bid_spread':[np.sum, np.max],
           'ask_spread':[np.sum, np.max],
           'total_volume':[np.sum, np.max],
           'volume_imbalance':[np.sum, np.max],
           'bid_ask_spread':[np.sum,  np.max],
   
     }

     create_feature_dict_time = {
         'log_return1': [realized_volatility],
         'log_return2': [realized_volatility],
         'log_return3': [realized_volatility],
         'log_return4': [realized_volatility],
     }

     # Function to get group stats for different windows (seconds in bucket)
     def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):
         # Group by the window
         df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()
         # Rename columns joining suffix
         df_feature.columns = ['_'.join(col) for col in df_feature.columns]
         # Add a suffix to differentiate windows
         if add_suffix:
             df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))
         return df_feature

        
     # Get the stats for different windows
     df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)
     df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)
     df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)
     df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)
     df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)
     df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)   

     # Merge all
     df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')
     df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')
     df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')
     df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')
     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')

     # Drop unnecesary time_ids
     df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)
     
     # Create row_id so we can merge
     stock_id = file_path.split('=')[1]
     df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')
     df_feature.drop(['time_id_'], axis = 1, inplace = True)

     
     return spark.createDataFrame(df_feature)

def trade_preprocessor(file_path):
    df= spark.read.parquet(file_path)

    df=df.withColumn('amount',  df['price']*df['size'])
    df = df.toPandas()
    #calculating log return(stock return) values
    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)
    #creating dictionaries for aggregating
    feature_dict= {
            'log_return':[realized_volatility],
            'seconds_in_bucket':[count_unique],
            'size':[np.sum,np.max,np.min],
            'order_count':[np.sum,np.max],
            'amount':[np.sum,np.max,np.min]
    }

    feature_dict_time={
        'log_return':[realized_volatility],
        'seconds_in_bucket':[count_unique],
        'size':[np.sum],
        'order_count':[np.sum]
    }

    #getting statistics for each group for different windows(seconds_in_bucket)
    def get_stats_window(feature_dict,seconds_in_bucket,suffix=False):
        df_feature =df[df['seconds_in_bucket']>=seconds_in_bucket].groupby('time_id').agg(feature_dict).reset_index()
        #df_feature =df['seconds_in_bucket'].groupby('time_id').agg(feature_dict).reset_index()
        #renaming the  columns
        df_feature.columns =['_'.join(col) for col in df_feature.columns]
        #adding the suffix for differentiating windows
        if suffix:
          df_feature=df_feature.add_suffix('_'+str(seconds_in_bucket))

        return df_feature

    df_feature=get_stats_window(feature_dict,seconds_in_bucket=0,suffix=False)
    df_feature_500=get_stats_window(feature_dict_time,seconds_in_bucket=500,suffix=True)
    df_feature_400=get_stats_window(feature_dict_time,seconds_in_bucket=400,suffix=True)
    df_feature_300=get_stats_window(feature_dict_time,seconds_in_bucket=300,suffix=True)
    df_feature_200=get_stats_window(feature_dict_time,seconds_in_bucket=200,suffix=True)
    df_feature_100=get_stats_window(feature_dict_time,seconds_in_bucket=100,suffix=True)
    
    def tendency(price,vol):
       df_diff =np.diff(price)
       val = (df_diff/price[1:])*100
       power =np.sum(val*vol[1:])      
       return power

    list_features=[]
    for time_id_n in df['time_id'].unique():
      df_id=df[df['time_id']== time_id_n]
      tendencyV=tendency(df_id['price'].values,df_id['size'].values)
      f_max=np.sum(df_id['price'].values > np.mean(df_id['price'].values))
      f_min=np.sum(df_id['price'].values < np.mean(df_id['price'].values))
      df_max=np.sum(np.diff(df_id['price'].values)>0)
      df_min=np.sum(np.diff(df_id['price'].values)<0)
      diff_p = df_id['price'].values - np.mean(df_id['price'].values)
      abs_diff_p = np.median(np.absolute(diff_p))
      energy_p=np.mean(df_id['price'].values**2)
      inter_quartile_p=np.percentile(df_id['price'].values,75)-np.percentile(df_id['price'].values,25)
      abs_diff_v = np.median(np.absolute(df_id['size'].values - np.mean(df_id['size'].values)))
      energy_v=np.mean(df_id['size'].values**2)
      inter_quartile_v=np.percentile(df_id['size'].values,75)-np.percentile(df_id['size'].values,25)

      
      list_features.append({'time_id':time_id_n, 'tendency':tendencyV, 'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,
                            'abs_diff_p':abs_diff_p,'energy_p':energy_p,'inter_quartile_p':inter_quartile_p,'abs_diff_v':abs_diff_v,
                            'energy_v':energy_v,'inter_quartile_v':inter_quartile_v })
      
   
    df_list = pd.DataFrame(list_features)
    df_feature=df_feature.merge(df_list,how='left',left_on='time_id_',right_on='time_id')

    #merging 
    df_feature=df_feature.merge(df_feature_500,how='left',left_on='time_id_',right_on='time_id__500')
    df_feature=df_feature.merge(df_feature_400,how='left',left_on='time_id_',right_on='time_id__400')
    df_feature=df_feature.merge(df_feature_300,how='left',left_on='time_id_',right_on='time_id__300')
    df_feature=df_feature.merge(df_feature_200,how='left',left_on='time_id_',right_on='time_id__200')
    df_feature=df_feature.merge(df_feature_100,how='left',left_on='time_id_',right_on='time_id__100')

    #dropping unnecessary time_id columns
    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'],axis=1,inplace=True)

    df_feature = df_feature.add_prefix('trade_')
    stock_id = file_path.split('=')[1]
    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x: f'{stock_id}-{x}')
    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)
    
   
    return spark.createDataFrame(df_feature)

from functools import reduce 
from pyspark.sql import DataFrame
def unionAll(*dfs):
    return reduce(DataFrame.unionAll, dfs)

from joblib import Parallel,delayed

def preprocessor(list_stock_ids, is_train = True):
  # Parrallel for loop
    def for_joblib(stock_id):
        id=row['stock_id']
        data_dir='/user/sbyrapu/input/optiver-realized-volatility-prediction/'
        
        file_path_book = data_dir + "book_train.parquet/stock_id=" +str(id)
        file_path_trade = data_dir + "trade_train.parquet/stock_id=" +str(id)
        
        if(is_train== True):
          book_df= book_preprocessor(file_path_book)
          trade_df= trade_preprocessor(file_path_trade)
          df_tmp = book_df.join(trade_df, book_df.row_id==trade_df.row_id, 'inner')
        else:
          book_df_test= book_preprocessor(file_path_book)
          trade_df_test= trade_preprocessor(file_path_trade) 
          df_tmp = book_df_test.join(trade_df_test, book_df_test.row_id==trade_df_test.row_id, 'inner')
        return df_tmp
  
    df_t = Parallel(n_jobs = None, verbose = 1)(delayed(for_joblib)(stock_id)for stock_id in list_stock_ids)
    df_t = unionAll(*df_t)

    return df_t

# Function to get group stats for the stock_id and time_id
def get_time_stock(df):
  
    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', 
                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', 
                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']

    # Group by the stock id
    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()
    # Rename columns joining suffix
    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]
    df_stock_id = df_stock_id.add_suffix('_' + 'stock')

    # Group by the stock id
    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()
    # Rename columns joining suffix
    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]
    df_time_id = df_time_id.add_suffix('_' + 'time')
    
    # Merge with original dataframe
    df = df.join(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])
    df = df.join(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])
    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)
    return df

# Preprocess them using Parallel and our single stock id functions
train_ = preprocessor(unique_stock_ids, is_train = True)
train = train.join(train_, on = ['row_id'], how = 'left')

# Preprocess them using Parallel and our single stock id functions
test_ = preprocessor(unique_stock_ids, is_train = False)
test = test.join(test_, on = ['row_id'], how = 'left')

# Get group stats of time_id and stock_id
train = get_time_stock(train) 
test = get_time_stock(test)

# Calculate spread
train=train.withColumn("size_tau",  np.sqrt(1 / train["trade_seconds_in_bucket_count_unique"]))
train=train.withColumn('size_tau_400',  np.sqrt(1 / train["trade_seconds_in_bucket_count_unique_400"]))
train=train.withColumn('size_tau_300',  np.sqrt( 1 / train["trade_seconds_in_bucket_count_unique_300"]))
train=train.withColumn("size_tau_200",  np.sqrt(1 / train["trade_seconds_in_bucket_count_unique_200"]))
train=train.withColumn('size_tau2',  np.sqrt(1 / train["trade_order_count_sum"]))
train=train.withColumn('size_tau2_400',np.sqrt(0.33 / train["trade_order_count_sum"]))
train=train.withColumn('size_tau2_300',np.sqrt(0.5 / train["trade_order_count_sum"]))
train=train.withColumn('size_tau2_200',train["size_tau2_400"] - train["size_tau2"])
train=train.withColumn('size_tau2_d',train["size_tau2_400"] - train["size_tau2"])


test=test.withColumn("size_tau",  np.sqrt(1 / test["trade_seconds_in_bucket_count_unique"]))
test=test.withColumn('size_tau_400',  np.sqrt(1 / test["trade_seconds_in_bucket_count_unique_400"]))
test=test.withColumn('size_tau_300',  np.sqrt( 1 / test["trade_seconds_in_bucket_count_unique_300"]))
test=test.withColumn("size_tau_200",  np.sqrt(1 / test["trade_seconds_in_bucket_count_unique_200"]))
test=test.withColumn('size_tau2',  np.sqrt(1 / test["trade_order_count_sum"]))
test=test.withColumn('size_tau2_400',np.sqrt(0.33 / test["trade_order_count_sum"]))
test=test.withColumn('size_tau2_300',np.sqrt(0.5 / test["trade_order_count_sum"]))
test=test.withColumn('size_tau2_200',train["size_tau2_400"] - test["size_tau2"])
test=test.withColumn('size_tau2_d',train["size_tau2_400"] - test["size_tau2"])

train.rdd.saveAsTextFile('train_pre_657.csv')
test.rdd.saveAsTextFile('test_pre_657.csv')