# -*- coding: utf-8 -*-
"""CS657_LightGBM_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BUNGk7LGDdDtPJmLogef35C1jpcVtcrH


!pip install pyspark

from google.colab import drive
drive.mount('/content/gdrive/', force_remount=True)

#!unzip gdrive/My\ Drive/optiver-realized-volatility-prediction.zip > /dev/null

!unzip gdrive/My\ Drive/preprocessed.zip > /dev/null """

import sys
import os
import math
import time
from pyspark import SparkConf, SparkContext
from pyspark.mllib.recommendation import ALS, Rating
from pyspark.sql import SparkSession 

# spark config
spark = SparkSession \
    .builder \
    .appName("movie recommendation") \
    .config("spark.driver.maxResultSize", "400g") \
    .config("spark.driver.memory", "400g") \
    .config("spark.executor.memory", "100g") \
    .config("spark.master", "local[12]") \
    .getOrCreate()
# get spark context
sc = spark.sparkContext
train = spark.read.option("header",True) \
     .csv("/content/preprocessed/train_v2.csv")
test = spark.read.option("header",True) \
     .csv("/content/preprocessed/test_v2.csv")

#train=train.limit(25000)
#train.count()

train,test=train.randomSplit([0.7, 0.3], seed=1)

from pyspark.sql.types import FloatType
import numpy as np
import lightgbm as lgb


def rmspe(y_true, y_pred):
    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))

def feval_rmspe(y_pred, lgb_train):
    y_true = lgb_train.get_label()
    return 'RMSPE', rmspe(y_true, y_pred), False

#features_list = numeric_inputs + string_inputs
input_columns = train.columns


for column in input_columns:
   train=train.withColumn(column, train[column].cast(FloatType()))
   test=test.withColumn(column, test[column].cast(FloatType()))



exclude_cols = 'time_id target row_id'.split()
features_lst = np.setdiff1d(train.columns, [exclude_cols]).tolist()
train_data,val_data=train.randomSplit([0.7, 0.3], seed=1)
label_key='target'

train.show()

"""**LightGBM**"""

x_train = np.array(train_data[features_lst].collect())
y_train = np.array(train_data.select(label_key).collect()).flatten()

x_val = np.array(val_data[features_lst].collect())
y_val = np.array(val_data.select(label_key).collect()).flatten()

x_test = np.array(test[features_lst].collect())

val_predictions = np.zeros(val_data.count())
# Create test array to store predictions
test_predictions = np.zeros(test.count())

len(x_train[0])

# Root mean squared percentage error weights
train_weights = 1 / np.square(y_train)
val_weights = 1 / np.square(y_val)

dtrain = lgb.Dataset(x_train, y_train, feature_name = features_lst, weight = train_weights)
deval = lgb.Dataset(x_val, y_val, feature_name = features_lst,weight = val_weights)

seed0=2021
params0 = {
    'objective': 'rmse',
    'boosting_type': 'gbdt',
    'max_depth': -1,
    'num_iterations':5000,
    'early_stopping_round': 200,
    'learning_rate': 0.05,
    'feature_fraction': 0.6,
    'verbose': 1,
    'nthread': 7,
    'train_metric': 'true',
    'max_bin':100,
    'min_data_in_leaf':500,
    'learning_rate': 0.05,
    'subsample': 0.72,
    'subsample_freq': 4,
    'feature_fraction': 0.5,
    'lambda_l1': 0.5,
    'lambda_l2': 1.0,
    'categorical_column':[0],
    'seed':seed0,
    'feature_fraction_seed': seed0,
    'bagging_seed': seed0,
    'drop_seed': seed0,
    'data_random_seed': seed0,
    'n_jobs':-1,
    'verbose': -1}

seed1=42
params1 = {
        'learning_rate': 0.25,        
        'lambda_l1': 2,
        'lambda_l2': 7,
        'num_leaves': 800,
        'num_iterations':5000,
        'min_sum_hessian_in_leaf': 20,
        'feature_fraction': 0.8,
        'feature_fraction_bynode': 0.8,
        'bagging_fraction': 0.9,
        'bagging_freq': 42,
        'min_data_in_leaf': 700,
        'max_depth': 4,
        'categorical_column':[0],
        'seed': seed1,
        'feature_fraction_seed': seed1,
        'bagging_seed': seed1,
        'drop_seed': seed1,
        'data_random_seed': seed1,
        'objective': 'rmse',
        'boosting': 'gbdt',
        'verbosity': -1,
        'n_jobs':-1,
    }

gbm = lgb.train(params0,
                dtrain, 
                valid_sets=[dtrain, deval],
                verbose_eval=params0['num_iterations']/20,
              )

gbm.save_model('lightgbm.model')

# Add predictions to the out of folds array
val_predictions = gbm.predict(x_val)
# Predict the test set
test_predictions = gbm.predict(x_test) 
rmspe_score = rmspe(y_val, val_predictions)

merged_list = [(float(y_val[i]), float(val_predictions[i])) for i in range(0, len(y_val))]
from pyspark.mllib.evaluation import RegressionMetrics
predictionAndObservations = sc.parallelize(merged_list)
metrics = RegressionMetrics(predictionAndObservations)
print(metrics.rootMeanSquaredError,metrics.r2,metrics.meanAbsoluteError)

#lgb.plot_importance(gbm,max_num_features=20)



gbm = lgb.train(params1,
                dtrain, 
                valid_sets=[dtrain, deval],
                verbose_eval=params1['num_iterations']/20,
              )

gbm.save_model('lightgbm.model')

# Add predictions to the out of folds array
val_predictions = gbm.predict(x_val)
# Predict the test set
test_predictions = gbm.predict(x_test) 
rmspe_score = rmspe(y_val, val_predictions)

merged_list = [(float(y_val[i]), float(val_predictions[i])) for i in range(0, len(y_val))]
from pyspark.mllib.evaluation import RegressionMetrics
predictionAndObservations = sc.parallelize(merged_list)
metrics = RegressionMetrics(predictionAndObservations)
print(metrics.rootMeanSquaredError,metrics.r2,metrics.meanAbsoluteError)

#lgb.plot_importance(gbm,max_num_features=20)
